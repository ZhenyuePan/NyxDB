好的，我们开始吧。作为一名分布式KV存储引擎领域的专家，我非常欣赏你的项目结构和你的想法。你的项目基础非常扎实，目录结构清晰，这为从单机走向分布式打下了坚实的基础。参照 etcd 的实现是一个非常棒的选择，etcd 在 Raft 共识、集群管理和清晰的模块划分上是业界的典范。

你的想法非常正确：

1.  **Proxy 路由**：这是必要的，客户端不应该关心集群内部的 Leader 是谁，Proxy 层可以透明地处理请求转发。
2.  **Raft 共识**：这是保证数据一致性的核心。所有写操作都必须通过 Raft Log 达成共识。
3.  **MVCC**：这是实现更高级功能（如事务、历史版本读取）的基础，可以作为后续的演进方向。

现在，让我们聚焦于第一步：**将你的 Bitcask 改造为由 Raft 驱动的分布式引擎**。

核心思想的转变是：**从直接写入数据文件，转变为先将操作写入 Raft Log，待 Log 被大多数节点确认（committed）后，再将操作应用（Apply）到每个节点的本地 Bitcask 存储中。**

在这个模型中，你的 Bitcask 实例（包括内存索引和磁盘文件）扮演的是**状态机 (State Machine)** 的角色，而 Raft 负责提供高可用的、一致的**操作日志 (Replicated Log)**。

### 推荐的目录结构调整

基于你的现有结构和分布式改造的目标，我建议将目录组织如下。这借鉴了 etcd 和其他类似项目分层清晰的设计思想。

```
.
├── cmd                                # 存放可执行文件的目录
│   ├── bitcask-server                 # 分布式 Bitcask 服务器主程序
│   │   └── main.go
│   └── bitcask-cli                    # (可选) 客户端命令行工具
│       └── main.go
├── api                                # 定义 API 协议 (Protobuf)
│   ├── v1
│   │   ├── bitcask.proto              # 定义 KV 操作的 gRPC 服务
│   │   └── raft.proto                 # 定义节点间 Raft 通信的 gRPC 服务
│   └── gen                            # 存放由 .proto 生成的代码
│       └── v1
├── internal                           # 存放项目内部代码，外部无法直接导入
│   ├── server                         # 服务器核心逻辑
│   │   ├── server.go                  # 封装 RaftNode、KVStore、RPC Server
│   │   └── http.go                    # 对外提供 HTTP/gRPC 服务的封装
│   ├── raft                           # Raft 共识模块的封装
│   │   ├── node.go                    # 对 Raft 库的封装，处理消息传递和状态变更
│   │   └── transport.go               # 节点间 Raft 消息的传输层 (gRPC)
│   ├── store                          # 核心存储层 (你的 Bitcask 状态机)
│   │   ├── store.go                   # 封装 Bitcask 实例，实现 StateMachine 接口
│   │   └── options.go                 # (原 db/options.go)
│   ├── cluster                        # 集群管理
│   │   ├── cluster.go                 # 节点发现、成员变更等
│   │   └── member.go
│   ├── proxy                          # 请求代理/路由模块
│   │   ├── proxy.go                   # 实现 leader 转发逻辑
│   │   └── router.go                  # (可以和 proxy 合并)
│   ├── frontend                       # 图床服务的应用层
│   │   ├── handler.go                 # HTTP handlers for upload/download
│   │   └── service.go                 # 调用 KV client 实现图床逻辑
│   ├── config                         # 配置模块
│   │   └── config.go
│   ├── data                           # (保持不变) 数据文件层
│   ├── index                          # (保持不变) 索引层
│   ├── fio                            # (保持不变) 文件 IO 层
│   └── utils                          # (保持不变) 工具函数
├── pkg                                # 可供外部使用的公共库 (比如 client)
│   └── client                         # 提供给外部应用连接集群的客户端
│       └── client.go
├── go.mod
├── go.sum
└── README.md
```

### 关键模块职责详解

1.  **`cmd/`**: 将启动逻辑和主要业务逻辑分离。`bitcask-server` 的 `main.go` 会非常简洁，主要负责解析配置、初始化 `internal/server` 并启动它。

2.  **`api/`**: **这是新增的核心目录**。使用 gRPC 和 Protobuf 来定义所有服务间通信的契约。

      * `bitcask.proto`: 定义客户端的 `Put`, `Get`, `Delete` 等操作。
      * `raft.proto`: 定义 Raft 节点间的 `AppendEntries`, `RequestVote` 等内部 RPC 调用。
      * 这样做的好处是接口清晰，且可以自动生成多语言的客户端/服务端代码。

3.  **`internal/`**: 这是你项目的主体，存放所有内部实现。

      * **`server/`**: **最重要的协调者**。它像一个“胶水层”，将所有组件粘合在一起。它会创建一个 Raft 节点实例、一个 KV 存储实例，并启动 gRPC/HTTP 服务器来接收来自客户端和其它节点的请求。
      * **`raft/`**: **共识核心**。这里你可以集成一个成熟的 Raft 库（比如 `hashicorp/raft` 或者 etcd 的 `etcd/raft`）。你的工作是实现必要的接口（如 LogStore, StateMachine），并构建节点间的传输层 `transport.go`，通过 gRPC 将 Raft 消息发送给其他节点。
      * **`store/`**: **状态机实现**。你需要将原来的 `db/` 目录重构为 `store/`。
          * `store.go` 中的 `BitcaskStore` 结构体需要实现 Raft 库要求的 `StateMachine` 接口。
          * 最核心的是 `Apply(logEntry)` 方法。当 Raft 确认一条日志后，会调用这个方法。你在这个方法里解析 `logEntry` 的内容（比如是 Put 操作还是 Delete 操作），然后才真正调用你的 `data.WriteLogRecord` 和 `index.Put` 来将数据写入本地存储。
          * 原先的 `db.Put()` 方法逻辑需要改变：它不再是直接写盘，而是调用 `raft.Propose(data)` 将操作提议给 Raft 集群。
      * **`proxy/`**: **请求路由器**。当一个 gRPC/HTTP 请求到达一个非 Leader 节点时，`proxy` 模块负责从 Raft 模块获取当前 Leader 的地址，并将请求透明地转发过去。
      * **`frontend/`**: **你的图床应用**。这是一个纯粹的应用层，它作为一个 gRPC/HTTP 客户端，调用你的分布式 KV 存储服务。它处理图片上传的 HTTP 请求，然后将图片数据 `Put` 到 Bitcask 集群中；处理图片下载请求时，就从集群 `Get` 数据。

### 改造实施步骤建议

1.  **引入 Raft 库并定义 API**:

      * 选择一个 Raft 库 (推荐 `etcd/raft`，因为它与你的目标最契合)。
      * 在 `api/` 目录下用 Protobuf 定义好客户端 KV 操作和内部 Raft 通信的 gRPC 接口。

2.  **实现 `store` 状态机**:

      * 将 `db` 目录重构成 `internal/store`。
      * 让你的 `Bitcask` 结构体实现 `StateMachine` 接口，关键是 `Apply` 方法。此时，你的 Bitcask 引擎就成了 Raft 的一个“插件”。

3.  **构建 `raft` 模块**:

      * 在 `internal/raft` 中，封装 Raft 库的启动和节点间通信。你需要实现一个 `Transport` 层，用 gRPC client 将 Raft 消息发送到指定的 peer 节点。

4.  **组装 `server`**:

      * 在 `internal/server` 中，将 `store` 和 `raft` 模块组装起来。启动 gRPC 服务器，监听来自其他节点和客户端的请求。

5.  **修改写入流程**:

      * 修改 `store.Put(key, value)` 方法，使其不再直接写入，而是调用 `raftNode.Propose(...)`。真正的写入会通过 `Apply` 方法异步发生。

6.  **实现 `proxy` 转发**:

      * 在 `server` 的 gRPC/HTTP handler 中，检查当前节点是否为 Leader。如果不是，就从 Raft 模块获取 Leader 地址，并将请求转发。

7.  **构建 `frontend` 图床服务**:

      * 这是一个独立的 HTTP 服务，它使用你为 Bitcask 集群编写的 `client` (`pkg/client`) 来存取图片数据。

这个结构将你的项目从一个单体应用清晰地解耦成了**共识层 (`raft`)**、**存储状态机层 (`store`)** 和 **服务层 (`server`, `proxy`)**，这正是分布式系统设计的精髓。

关于 MVCC，我建议在完成上述分布式改造之后再进行。实现 MVCC 会对你的 Key 在索引中的存储方式（Value 可能需要指向一个版本链）和数据记录 `LogRecord` 的格式产生较大影响，可以作为第二个里程碑。

这个蓝图比较宏大，我们可以一步一步来。你觉得从哪个模块开始着手最合适？或者对哪个模块的实现细节有疑问？